# Comparaison des Mod√®les d'Embedding Open Source

## Crit√®res pour OHADA
- ‚úÖ Maximum de tokens (documents longs : 8k-130k chars)
- ‚úÖ Excellente qualit√© pour le fran√ßais
- ‚úÖ Open source et local
- ‚úÖ Performance optimale

## Top 3 Mod√®les Recommand√©s

### ü•á 1. BAAI/bge-m3 (RECOMMAND√â)
```
Mod√®le: BAAI/bge-m3
Tokens max: 8192 tokens
Dimension: 1024
Langues: Multilingue (100+ langues, excellent fran√ßais)
Performance: #1 sur MTEB multilingue
Taille: ~2.3 GB
License: MIT
```

**Avantages:**
- ‚úÖ **8192 tokens** ‚Üí G√®re les longs documents OHADA
- ‚úÖ **Multilingue state-of-the-art** ‚Üí Excellent pour fran√ßais juridique
- ‚úÖ **Dimension 1024** ‚Üí Bon compromis qualit√©/performance
- ‚úÖ **#1 MTEB benchmark** ‚Üí Meilleure qualit√© g√©n√©rale
- ‚úÖ Support multi-fonctionnalit√© (dense, sparse, colBERT)

**Inconv√©nients:**
- ‚ö†Ô∏è Taille importante (~2.3 GB)

**Benchmark MTEB (fran√ßais):**
- Retrieval: 55.4
- Classification: 69.8
- Clustering: 46.2

---

### ü•à 2. jinaai/jina-embeddings-v2-base-fr
```
Mod√®le: jinaai/jina-embeddings-v2-base-fr
Tokens max: 8192 tokens
Dimension: 768
Langues: Fran√ßais optimis√©
Performance: Excellent pour fran√ßais
Taille: ~550 MB
License: Apache 2.0
```

**Avantages:**
- ‚úÖ **8192 tokens** ‚Üí G√®re les longs documents
- ‚úÖ **Optimis√© fran√ßais** ‚Üí Sp√©cifique domaine francophone
- ‚úÖ Plus l√©ger que BGE-M3
- ‚úÖ Flash Attention 2 (rapide)

**Inconv√©nients:**
- ‚ö†Ô∏è Dimension 768 (vs 1024 pour BGE-M3)
- ‚ö†Ô∏è Moins polyvalent que BGE-M3

---

### ü•â 3. intfloat/multilingual-e5-large
```
Mod√®le: intfloat/multilingual-e5-large
Tokens max: 512 tokens
Dimension: 1024
Langues: Multilingue (94 langues)
Performance: Tr√®s bon pour fran√ßais
Taille: ~2.2 GB
License: MIT
```

**Avantages:**
- ‚úÖ Excellent qualit√© multilingue
- ‚úÖ Dimension 1024
- ‚úÖ Bien test√© et stable

**Inconv√©nients:**
- ‚ùå **Seulement 512 tokens** ‚Üí N√©cessite chunking agressif
- ‚ö†Ô∏è Moins performant que BGE-M3

---

## üìä Tableau Comparatif

| Mod√®le | Tokens Max | Dimension | Fran√ßais | Taille | Vitesse |
|--------|-----------|-----------|----------|--------|---------|
| **BGE-M3** ‚≠ê | **8192** | 1024 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 2.3 GB | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Jina-v2-fr | 8192 | 768 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 550 MB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| E5-large | 512 | 1024 | ‚≠ê‚≠ê‚≠ê‚≠ê | 2.2 GB | ‚≠ê‚≠ê‚≠ê |

---

## ‚úÖ Recommandation Finale : **BAAI/bge-m3**

### Pourquoi BGE-M3 pour OHADA ?

1. **Gestion des longs documents**
   - Documents OHADA : 3k-130k chars (~750-32k tokens)
   - BGE-M3 : 8192 tokens ‚Üí R√©duit le chunking

2. **Qualit√© fran√ßaise exceptionnelle**
   - Entra√Æn√© sur corpus multilingue massif
   - Performance √©tat de l'art sur benchmark fran√ßais

3. **Polyvalence**
   - Dense retrieval (recherche s√©mantique)
   - Sparse retrieval (BM25-like)
   - Multi-vector (colBERT-style)

4. **Production-ready**
   - Tr√®s utilis√© en production
   - Excellente documentation
   - Support communautaire actif

### Configuration Recommand√©e

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer(
    'BAAI/bge-m3',
    device='cpu'  # ou 'cuda' si GPU disponible
)

# Pour documents longs
embeddings = model.encode(
    texts,
    batch_size=4,
    show_progress_bar=True,
    normalize_embeddings=True,  # Important pour similarit√© cosinus
    max_length=8192  # Utiliser toute la capacit√©
)
```

### Alternative pour ressources limit√©es

Si m√©moire/CPU limit√©e ‚Üí **jina-embeddings-v2-base-fr** :
- Plus l√©ger (550 MB vs 2.3 GB)
- Plus rapide
- Toujours 8192 tokens

---

## üöÄ Prochaines √âtapes

1. ‚úÖ Installer sentence-transformers
2. ‚úÖ T√©l√©charger BGE-M3
3. ‚úÖ Configurer ChromaDB avec BGE-M3
4. ‚úÖ Cr√©er pipeline de chunking (cibles : 2000-4000 tokens/chunk)
5. ‚úÖ Vectoriser les 215 documents
6. ‚úÖ Tester la recherche

## üìö Sources

- MTEB Leaderboard: https://huggingface.co/spaces/mteb/leaderboard
- BGE-M3: https://huggingface.co/BAAI/bge-m3
- Jina v2: https://huggingface.co/jinaai/jina-embeddings-v2-base-fr
- E5: https://huggingface.co/intfloat/multilingual-e5-large
