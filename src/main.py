"""
Main entry point for the OHADA Expert Accounting system.
Version optimis√©e avec configuration flexible des mod√®les de langage et gestion de l'environnement.
"""

import os
import sys
import time
import threading
import signal
import logging
import yaml
from pathlib import Path
from dotenv import load_dotenv
from typing import Dict, Any, Optional, List

# Load environment variables
load_dotenv()

# D√©terminer l'environnement et le chemin de configuration
ENVIRONMENT = os.getenv("OHADA_ENV", "test")
CONFIG_PATH = os.getenv("OHADA_CONFIG_PATH", "./src/config")
CONFIG_FILE = os.path.join(CONFIG_PATH, f"llm_config_{ENVIRONMENT}.yaml")

# Configuration du logging
logging.basicConfig(
    level=logging.INFO if ENVIRONMENT == "production" else logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f"ohada_main_{ENVIRONMENT}.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("ohada_main")

# Constantes
DEFAULT_TIMEOUT = 180  # Secondes

def load_llm_config():
    """Charge la configuration des mod√®les de langage depuis le fichier YAML"""
    try:
        logger.info(f"Chargement de la configuration depuis {CONFIG_FILE}")
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except Exception as e:
        logger.error(f"Erreur lors du chargement de la configuration: {e}")
        # Essayer avec le chemin par d√©faut si le fichier n'est pas trouv√©
        default_config = f"./src/config/llm_config.yaml"
        if os.path.exists(default_config):
            logger.info(f"Tentative avec le fichier par d√©faut: {default_config}")
            try:
                with open(default_config, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                return config
            except Exception as e2:
                logger.error(f"Erreur lors du chargement de la configuration par d√©faut: {e2}")
        return None

def print_welcome():
    """Affiche le message de bienvenue et les instructions"""
    # Logo diff√©rent selon l'environnement
    if ENVIRONMENT == "production":
        env_indicator = "üíº [PRODUCTION]"
    else:
        env_indicator = "üß™ [TEST]"
        
    print("\n" + "=" * 80)
    print(f"                  OHADA EXPERT-COMPTABLE AI {env_indicator}".center(80))
    print("=" * 80)
    print("\nBienvenue dans votre assistant d'expertise comptable OHADA!")
    print("Posez des questions sur le plan comptable, les normes et r√®glements OHADA.")
    print("\nExemples de questions:")
    print("  - Comment fonctionne l'amortissement d√©gressif dans le SYSCOHADA?")
    print("  - Expliquez la structure du plan comptable OHADA.")
    print("  - Quelles sont les r√®gles pour la comptabilisation des subventions?")
    print("\nTapez 'exit', 'quit', ou 'q' pour quitter.")
    print("-" * 80)

def process_query_with_extended_timeout(query: str, api=None, max_wait_time: int = DEFAULT_TIMEOUT) -> Dict[str, Any]:
    """
    Traite une requ√™te avec gestion de timeout √©tendu
    
    Args:
        query: Question de l'utilisateur
        api: Instance de OhadaQueryAPI (peut √™tre None pour cr√©ation √† la demande)
        max_wait_time: Temps d'attente maximum en secondes
        
    Returns:
        Dictionnaire contenant la r√©ponse et les informations de timing
    """
    # Import n√©cessaire seulement quand n√©cessaire (pour un d√©marrage plus rapide)
    try:
        # Import du module de requ√™te
        from src.retrieval.ohada_hybrid_retriever import create_ohada_query_api
    except ImportError as e:
        logger.error(f"Erreur lors de l'importation des modules: {e}")
        return {
            "response": f"Erreur lors de l'importation des modules: {str(e)}",
            "elapsed_time": 0,
            "success": False
        }
    
    # Variable de r√©sultat partag√©e pour la communication entre threads
    result = {
        "response": None,
        "done": False,
        "success": False,
        "search_time": 0,
        "generation_time": 0,
        "elapsed_time": 0,
        "error": None
    }
    
    # Indicateur pour l'annulation par l'utilisateur
    user_cancelled = {"value": False}
    
    # Fonction qui s'ex√©cutera dans un thread s√©par√©
    def process_thread():
        try:
            start_time = time.time()
            
            # Cr√©er ou utiliser l'instance d'API fournie
            nonlocal api
            if api is None:
                logger.info("Cr√©ation d'une nouvelle instance d'API")
                api = create_ohada_query_api(config_path=CONFIG_PATH)
            
            # Analyser l'intention pour d√©terminer si c'est une requ√™te conversationnelle
            from src.generation.intent_classifier import LLMIntentAnalyzer
            
            # Initialiser l'analyseur d'intention
            intent_analyzer = LLMIntentAnalyzer(
                llm_client=api.llm_client,
                assistant_config=api.llm_config.get_assistant_personality()
            )
            
            # Analyser l'intention de la requ√™te
            intent, metadata, direct_response = analyze_intent(query, intent_analyzer)
            
            # Si une r√©ponse directe est disponible, l'utiliser
            if direct_response:
                logger.info(f"R√©ponse directe g√©n√©r√©e pour l'intention: {intent}")
                result["response"] = direct_response
                result["intent"] = intent
                result["intent_analysis"] = True
                result["elapsed_time"] = time.time() - start_time
                result["success"] = True
                result["done"] = True
                return
            
            # Sinon, ex√©cuter la recherche de connaissances normalement
            query_result = api.search_ohada_knowledge(
                query=query,
                n_results=3,
                include_sources=True
            )
            
            # Extraire la r√©ponse et les m√©triques de performance
            result["response"] = query_result.get("answer", "")
            result["search_time"] = query_result.get("performance", {}).get("search_time_seconds", 0)
            result["generation_time"] = query_result.get("performance", {}).get("generation_time_seconds", 0)
            result["elapsed_time"] = time.time() - start_time
            result["success"] = True
            result["done"] = True
            
        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            result["response"] = f"D√©sol√©, une erreur s'est produite lors du traitement de votre question: {str(e)}"
            result["error"] = error_details
            result["elapsed_time"] = time.time() - start_time
            result["done"] = True
            logger.error(f"Erreur dans le thread de traitement: {error_details}")
    
    # Cr√©er et d√©marrer le thread
    thread = threading.Thread(target=process_thread)
    thread.daemon = True  # Le thread sera tu√© si le programme principal se termine
    thread.start()
    
    # Fonction pour g√©rer l'entr√©e utilisateur dans un thread s√©par√©
    def input_thread():
        while not result["done"] and not user_cancelled["value"]:
            try:
                user_input = input("Appuyez sur 'C' pour annuler ou attendez la r√©ponse compl√®te: ")
                if user_input.lower() == 'c':
                    user_cancelled["value"] = True
                    print("Vous avez choisi d'annuler. Veuillez patienter pendant que nous finalisons...")
                    break
            except:
                # Ignorer les erreurs d'entr√©e
                time.sleep(1)
                continue
    
    # Attendre que le thread se termine avec des v√©rifications p√©riodiques
    wait_interval = 5   # V√©rifier toutes les 5 secondes
    elapsed = 0
    reminder_intervals = [30, 60, 90, 120]  # Secondes o√π rappeler √† l'utilisateur qu'il peut annuler
    
    # D√©marrer le thread d'entr√©e apr√®s le premier intervalle d'attente
    time.sleep(wait_interval)
    elapsed += wait_interval
    
    if not result["done"]:
        print(f"‚è≥ Traitement en cours ({elapsed}s)... Veuillez patienter.")
        input_thread = threading.Thread(target=input_thread)
        input_thread.daemon = True
        input_thread.start()
    
    # Continuer √† attendre avec des mises √† jour p√©riodiques
    while not result["done"] and elapsed < max_wait_time and not user_cancelled["value"]:
        thread.join(wait_interval)
        elapsed += wait_interval
        if not result["done"]:
            print(f"‚è≥ Traitement en cours ({elapsed}s)... Veuillez patienter.")
            
            # √Ä certains intervalles, rappeler √† l'utilisateur qu'il peut annuler
            if elapsed in reminder_intervals:
                print("La g√©n√©ration prend plus de temps que pr√©vu. Pour une r√©ponse de qualit√©, veuillez patienter.")
                print("Vous pouvez appuyer sur 'C' pour annuler et obtenir une r√©ponse partielle.")
    
    # Si l'utilisateur a annul√©
    if user_cancelled["value"]:
        # Attendre un peu pour voir si le thread se termine quand m√™me
        thread.join(5)
        
        if result["done"]:
            # Le thread s'est termin√© malgr√© l'annulation
            print("La r√©ponse compl√®te vient d'√™tre g√©n√©r√©e malgr√© l'annulation!")
        else:
            print("G√©n√©ration d'une r√©ponse partielle...")
            fallback_response = generate_fallback_response(query)
            
            return {
                "response": fallback_response,
                "elapsed_time": elapsed,
                "success": False,
                "cancelled": True
            }
    
    # Si le thread est toujours en cours d'ex√©cution apr√®s max_wait_time
    if not result["done"]:
        print(f"\n‚ö†Ô∏è La g√©n√©ration a atteint le temps maximum autoris√© de {max_wait_time//60} minutes.")
        print("Nous allons quand m√™me continuer √† attendre la r√©ponse compl√®te...")
        
        # Continuer √† attendre ind√©finiment avec des mises √† jour toutes les 30 secondes
        extra_wait = 0
        extra_wait_limit = 300  # Maximum 5 minutes suppl√©mentaires
        while not result["done"] and extra_wait < extra_wait_limit:
            thread.join(30)
            extra_wait += 30
            if not result["done"]:
                print(f"‚è≥ Toujours en attente... ({elapsed + extra_wait}s). Appuyez sur 'C' pour abandonner.")
        
        if not result["done"]:
            print(f"\n‚ö†Ô∏è Abandon apr√®s {(elapsed + extra_wait)//60} minutes d'attente.")
            fallback_response = generate_fallback_response(query)
            
            return {
                "response": fallback_response,
                "elapsed_time": elapsed + extra_wait,
                "success": False,
                "timeout": True
            }
    
    return result

def analyze_intent(query: str, intent_analyzer):
    """
    Analyse l'intention d'une requ√™te et g√©n√®re une r√©ponse directe si n√©cessaire
    
    Args:
        query: Requ√™te de l'utilisateur
        intent_analyzer: Analyseur d'intention
    
    Returns:
        Tuple (intention, m√©tadonn√©es, r√©ponse directe ou None)
    """
    # Analyser l'intention de la requ√™te
    intent, metadata = intent_analyzer.analyze_intent(query)
    
    # Enrichir les m√©tadonn√©es avec la requ√™te originale pour r√©f√©rence future
    metadata["query"] = query
    
    # G√©n√©rer une r√©ponse directe si n√©cessaire
    direct_response = intent_analyzer.generate_response(intent, metadata)
    
    return intent, metadata, direct_response

def generate_fallback_response(query: str) -> str:
    """
    G√©n√®re une r√©ponse de secours lorsque le processus principal est annul√© ou expire.
    Utilise la m√™me configuration que le syst√®me principal.
    
    Args:
        query: Question de l'utilisateur
        
    Returns:
        Une r√©ponse simplifi√©e
    """
    try:
        # Import des modules n√©cessaires
        from src.config.ohada_config import LLMConfig
        from src.utils.ohada_clients import LLMClient
        
        # Charger la configuration des mod√®les
        llm_config = LLMConfig(CONFIG_PATH)
        
        # Initialiser le client LLM
        llm_client = LLMClient(llm_config)
        
        # G√©n√©rer une r√©ponse simplifi√©e
        prompt = f"R√©pondez bri√®vement √† cette question sur le plan comptable OHADA (maximum 5 paragraphes): {query}"
        
        fallback_response = llm_client.generate_response(
            system_prompt="Vous √™tes un expert-comptable sp√©cialis√© dans le plan comptable OHADA. R√©pondez de mani√®re concise et pr√©cise.",
            user_prompt=prompt,
            max_tokens=500,
            temperature=0.3
        )
        
        if fallback_response:
            return f"‚ö†Ô∏è R√©ponse partielle :\n\n{fallback_response}"
        else:
            return "‚ö†Ô∏è Le temps de r√©ponse a d√©pass√© la limite. Veuillez poser une question plus sp√©cifique pour obtenir une r√©ponse plus rapide."
            
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration de la r√©ponse de secours: {e}")
        return f"‚ö†Ô∏è Le temps de r√©ponse a d√©pass√© la limite. Veuillez poser une question plus sp√©cifique pour obtenir une r√©ponse plus rapide.\n\nErreur: {str(e)}"

def check_provider_keys(config: Dict) -> Dict[str, bool]:
    """
    V√©rifie quelles cl√©s API sont disponibles pour chaque fournisseur
    
    Args:
        config: Configuration des mod√®les de langage
        
    Returns:
        Dictionnaire des fournisseurs avec leur statut de disponibilit√©
    """
    providers_status = {}
    
    if not config or "providers" not in config:
        return providers_status
    
    for provider_name, provider_config in config["providers"].items():
        # Ignorer les fournisseurs d√©sactiv√©s
        if provider_config.get("enabled") is False:
            providers_status[provider_name] = False
            continue
        
        # V√©rifier si une cl√© API est n√©cessaire (certains mod√®les locaux pourraient ne pas en avoir besoin)
        api_key_env = provider_config.get("api_key_env")
        if not api_key_env:
            # Pour les fournisseurs sans cl√© API (comme les mod√®les locaux)
            providers_status[provider_name] = True
            continue
        
        # V√©rifier si la cl√© API est d√©finie
        api_key = os.getenv(api_key_env)
        providers_status[provider_name] = bool(api_key)
    
    return providers_status

def check_environment() -> bool:
    """
    V√©rifie si les variables d'environnement n√©cessaires sont d√©finies
    
    Returns:
        True si au moins une cl√© API requise est disponible
    """
    # Charger la configuration
    config = load_llm_config()
    if not config:
        logger.error("Impossible de charger la configuration des mod√®les")
        return False
    
    # V√©rifier les cl√©s API
    providers_status = check_provider_keys(config)
    
    if not providers_status:
        logger.error("Aucun fournisseur configur√©!")
        return False
    
    # V√©rifier les fournisseurs prioritaires pour les r√©ponses
    response_providers = []
    if "provider_priority" in config:
        response_providers = config["provider_priority"]
    elif "default_provider" in config:
        response_providers = [config["default_provider"]]
    
    # V√©rifier les fournisseurs prioritaires pour les embeddings
    embedding_providers = []
    if "embedding_provider_priority" in config:
        embedding_providers = config["embedding_provider_priority"]
    elif "default_embedding_provider" in config:
        embedding_providers = [config["default_embedding_provider"]]
    
    # V√©rifier si au moins un fournisseur est disponible pour chaque fonction
    response_available = any(providers_status.get(p, False) for p in response_providers)
    embedding_available = any(providers_status.get(p, False) for p in embedding_providers)
    
    # Afficher les informations sur les fournisseurs disponibles
    print(f"\n=== Configuration des mod√®les ({ENVIRONMENT}) ===")
    print("Fournisseurs pour les r√©ponses:")
    for p in response_providers:
        status = "‚úÖ Disponible" if providers_status.get(p, False) else "‚ùå Non disponible"
        print(f"  - {p}: {status}")
    
    print("\nFournisseurs pour les embeddings:")
    for p in embedding_providers:
        status = "‚úÖ Disponible" if providers_status.get(p, False) else "‚ùå Non disponible"
        print(f"  - {p}: {status}")
    
    print("\n" + "-" * 80)
    
    if not response_available:
        logger.error("‚ùå Aucun fournisseur disponible pour les r√©ponses!")
    
    if not embedding_available:
        logger.error("‚ùå Aucun fournisseur disponible pour les embeddings!")
    
    return response_available and embedding_available

def main():
    """Fonction principale pour ex√©cuter le syst√®me OHADA Expert Accounting"""
    print(f"\nInitialisation de l'Assistant Expert-Comptable OHADA ({ENVIRONMENT})...")
    
    # V√©rifier les variables d'environnement et la configuration
    if not check_environment():
        print("‚ùå Erreur: Configuration invalide ou cl√©s API manquantes.")
        print("Veuillez v√©rifier votre fichier de configuration et vos variables d'environnement.")
        print("Au moins un fournisseur de mod√®le doit √™tre correctement configur√© pour chaque fonction.")
        return
    
    print_welcome()
    
    # Pr√©charger le mod√®le d'embedding au d√©marrage
    try:
        print("Pr√©chargement du mod√®le d'embedding (cela peut prendre un moment)...")
        from src.vector_db.ohada_vector_db_structure import OhadaEmbedder
        # Charger le mod√®le en utilisant le constructeur (qui va maintenant utiliser un singleton)
        
        # D√©terminer le mod√®le √† utiliser selon l'environnement
        if ENVIRONMENT == "production":
            embedding_model = "Alibaba-NLP/gte-Qwen2-1.5B-instruct"
        else:
            embedding_model = "all-MiniLM-L6-v2"
            
        embedder = OhadaEmbedder(model_name=embedding_model)
        # G√©n√©rer un petit embedding pour s'assurer que tout fonctionne
        _ = embedder.generate_embedding("Test de pr√©chargement")
        print(f"Mod√®le d'embedding {embedding_model} pr√©charg√© avec succ√®s.")
    except Exception as e:
        logger.error(f"Erreur lors du pr√©chargement du mod√®le d'embedding: {e}")
        print(f"‚ö†Ô∏è Avertissement: Le pr√©chargement du mod√®le d'embedding a √©chou√©: {str(e)}")
        print("Le mod√®le sera charg√© lors de la premi√®re requ√™te.\n")
    
    # Cr√©er l'instance d'API une seule fois (sera r√©utilis√©e)
    try:
        from src.retrieval.ohada_hybrid_retriever import create_ohada_query_api
        print("Initialisation de l'API de requ√™te...")
        api = create_ohada_query_api(config_path=CONFIG_PATH)
        logger.info("API initialis√©e avec succ√®s")
        print("API initialis√©e avec succ√®s.\n")
    except Exception as e:
        logger.error(f"Erreur lors de l'initialisation de l'API: {e}")
        print(f"‚ö†Ô∏è Avertissement: {str(e)}")
        print("L'API sera initialis√©e √† la demande pour chaque requ√™te.\n")
        api = None  # Sera cr√©√©e √† la demande dans chaque requ√™te
    
    # Boucle d'interaction principale
    while True:
        # Obtenir la requ√™te de l'utilisateur
        user_query = input("\nüí¨ Votre question (ou 'exit' pour quitter): ")
        
        # V√©rifier si l'utilisateur veut quitter
        if user_query.lower() in ["exit", "quit", "q", "quitter"]:
            print("\nMerci d'avoir utilis√© l'Assistant Expert-Comptable OHADA. √Ä bient√¥t!")
            break
        
        if not user_query.strip():
            print("Veuillez entrer une question valide.")
            continue
        
        # Traiter la requ√™te avec timeout √©tendu
        try:
            print("\n‚è≥ Recherche d'informations en cours...")
            start_time = time.time()
            
            # Traiter la requ√™te avec gestion de timeout √©tendu
            result = process_query_with_extended_timeout(user_query, api=api)
            
            # Obtenir la r√©ponse
            response = result.get("response", "")
            
            # Calculer le temps √©coul√©
            elapsed_time = result.get("elapsed_time", time.time() - start_time)
            
            # Afficher la r√©ponse
            print("\n" + "-" * 80)
            
            # Si c'est une r√©ponse bas√©e sur l'intention, afficher l'intention d√©tect√©e
            if result.get("intent_analysis", False):
                print(f"‚úÖ R√©ponse (g√©n√©r√©e en {elapsed_time:.2f} secondes, intention: {result.get('intent', 'inconnue')}):")
            else:
                print(f"‚úÖ R√©ponse (g√©n√©r√©e en {elapsed_time:.2f} secondes):")
                
            print("-" * 80 + "\n")
            print(response)
            print("\n" + "-" * 80)
            
            # Afficher l'erreur √©ventuelle
            if result.get("error") and not result.get("success", True):
                print("\n‚ö†Ô∏è Note: La r√©ponse a √©t√© g√©n√©r√©e malgr√© une erreur sous-jacente.")
            
        except Exception as e:
            logger.error(f"Une erreur s'est produite: {e}")
            print(f"\n‚ùå Une erreur s'est produite: {str(e)}")
            print("Veuillez r√©essayer avec une autre question.")
            
            # Afficher l'erreur d√©taill√©e en environnement de d√©veloppement
            if os.getenv("OHADA_ENV") == "development":
                import traceback
                traceback.print_exc()

if __name__ == "__main__":
    main()